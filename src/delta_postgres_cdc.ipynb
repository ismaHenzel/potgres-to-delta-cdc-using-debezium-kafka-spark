{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from itertools import cycle\n",
    "\n",
    "from delta import DeltaTable\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import  functions as F\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.streaming.query import StreamingQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"postgres_delta_cdc\")\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "log_format = logging.Formatter(\n",
    "    fmt=f\"%(levelname)s %(asctime)s  - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Adding file and console handlers for logging.\n",
    "\n",
    "file_handler = logging.FileHandler(filename='/app/logs/streaming.log')\n",
    "file_handler.setFormatter(log_format)\n",
    "file_handler.setLevel(level=logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "console_handler.setFormatter(log_format)\n",
    "console_handler.setLevel(level=logging.DEBUG)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,org.apache.spark:spark-avro_2.12:3.4.0,io.delta:delta-core_2.12:2.4.0\")\n",
    "    .config(\"spark.jars.repositories\",\"https://mvnrepository.com/artifact/io.delta/delta-core,https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core,https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10,https://mvnrepository.com/artifact/org.apache.spark/spark-avro\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_registry_latest(topic:str)->str:\n",
    "    '''Get the Avro JSON schema for the current topic.'''\n",
    "    response = requests.get(f'http://host.docker.internal:8081/subjects/{topic}-value/versions/latest/schema')\n",
    "    return json.dumps(response.json())\n",
    "\n",
    "def desserialize_avro_column(df:DataFrame,schema:str)->DataFrame:\n",
    "    '''Remove confluent extra 5 bytes, then transform the Avro column.'''\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"value\", F.expr(\"substring(value, 6, length(value)-5)\"))\n",
    "        .withColumn(\"value\", from_avro('value',jsonFormatSchema=schema))\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avaiable_tables(table_properties:list)->list:\n",
    "    return list(map(lambda x: x.get('Table'),table_properties))\n",
    "\n",
    "def get_kafka_topics(table_properties:list)-> str:\n",
    "    return ','.join(get_avaiable_tables(table_properties=table_properties))\n",
    "\n",
    "def get_table_properites(table_properties:str, table:str):\n",
    "    return list(filter(lambda x: x.get('Table')==table, table_properties))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreachBatch(\n",
    "        df_cdc_read_stream_batch:DataFrame,\n",
    "        batch_id:int,\n",
    "        topic:str,\n",
    "        table_schema:str\n",
    "        ,partition_window:Window, \n",
    "        join_query:str\n",
    "    )->None:\n",
    "    \"\"\"\n",
    "     For each batch of CDC (Change Data Capture), deserialize the Avro column and retrieve the highest operation done, partitioned by the available table primary keys.\n",
    "\n",
    "    Args:\n",
    "        df_cdc_read_stream_batch (DataFrame): Current batch Spark DataFrame.\n",
    "        batch_id (int): Current batch ID of the streaming table.\n",
    "        topic (str): Kafka topic name.\n",
    "        table_schema (str): Table schema registry Avro-dumped JSON.\n",
    "        partition_window (Window): Window that groups CDC rows to get the last operation based on primary keys.\n",
    "        join_query (str): SQL query to match the primary keys.\n",
    "    \"\"\"     \n",
    "\n",
    "    df_cdc_read_stream = (\n",
    "        desserialize_avro_column(df_cdc_read_stream_batch, table_schema)\n",
    "        .withColumn('values', \n",
    "            F.when(\n",
    "                F.col('value.op')!='d',\n",
    "                F.col('value.after')\n",
    "            ).otherwise(\n",
    "                F.col('value.before')\n",
    "            )\n",
    "        )\n",
    "        .select('values.*', F.col('value.op').alias('last_operation'), F.col('value.ts_ms').alias('ts_ms'))\n",
    "        .withColumn('max_value', F.row_number().over(partition_window))\n",
    "        .filter(F.col('max_value') == 1)\n",
    "        .drop('max_value', 'ts_ms')\n",
    "    )\n",
    "\n",
    "    if not DeltaTable.isDeltaTable(spark, f'/app/data/{topic}/'):\n",
    "        logger.info(f'Saving topic {topic} for the first time')\n",
    "        df_cdc_read_stream.write.format('delta').save(f'/app/data/{topic}/') \n",
    "    else:\n",
    "        logger.info(f'Saving a new batch for the topic {topic}')\n",
    "\n",
    "        deltaTable = DeltaTable.forPath(spark,f'/app/data/{topic}/')\n",
    "        \n",
    "        deltaTable.alias(\"stored_val\").merge(\n",
    "            df_cdc_read_stream.alias(\"new_val\"),\n",
    "            join_query\n",
    "        )\\\n",
    "        .whenMatchedDelete(\n",
    "            condition = \"new_val.last_operation = 'd'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll(\n",
    "            condition=\"new_val.last_operation = 'u'\",\n",
    "        )\\\n",
    "        .whenNotMatchedInsertAll(\n",
    "            condition = \"new_val.last_operation != 'd'\", # if delete was the last operation, do nothing\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgres_delta_cdc_table(current_table:str,df_cdc_read_stream:DataFrame,table_properties:dict)-> StreamingQuery:\n",
    "    \"\"\" \n",
    "    Function that creates a Spark WriteStream for the tables.\n",
    "\n",
    "    Args:\n",
    "        current_table (str): The current CDC table name.\n",
    "        df_cdc_read_stream (DataFrame): The readstream Spark DataFrame.\n",
    "        table_properties (dict): A metadata dictionary to obtain table properties.\n",
    "\n",
    "    Returns:\n",
    "        StreamingQuery: The Spark streaming query.\n",
    "    \"\"\" \n",
    "\n",
    "    table_schema = get_schema_registry_latest(current_table)\n",
    "    current_table_properties = get_table_properites(table_properties, current_table)\n",
    "    processing_time = current_table_properties.get('ProcessingTime')  \n",
    "    table_primary_keys = current_table_properties.get('PrimaryKeys')\n",
    "    join_query = 'AND'.join([f'stored_val.{val} = new_val.{val}' for val in table_primary_keys])\n",
    "    partition_window =  Window.partitionBy(*table_primary_keys).orderBy(F.col('ts_ms').desc())\n",
    "\n",
    "    current_table_properties\n",
    "    return df_cdc_read_stream\\\n",
    "        .filter(F.col(\"topic\")==current_table)\\\n",
    "        .writeStream\\\n",
    "        .format(\"delta\")\\\n",
    "        .foreachBatch(lambda  batch, batchId: foreachBatch(batch, batchId, topic=current_table,table_schema=table_schema, partition_window=partition_window, join_query=join_query))\\\n",
    "        .option(\"checkpointLocation\",f'/app/data/{current_table}/_checkpoints')\\\n",
    "        .queryName(current_table)\\\n",
    "        .trigger(processingTime=processing_time)\\\n",
    "        .start(f'/app/data/{current_table}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting metadata\n",
    "\n",
    "table_properties = json.load(open('/app/metadata/table_properties.json'))\n",
    "kafka_bootstrap_server = 'host.docker.internal:29092'\n",
    "kafka_topics = get_kafka_topics(table_properties)\n",
    "kafka_schema = get_schema_registry_latest(kafka_topics)\n",
    "avaiable_tables = get_avaiable_tables(table_properties)\n",
    "\n",
    "# Starting the readstream for multiple topics\n",
    "\n",
    "df_cdc_read_stream = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_server)\n",
    "    .option(\"subscribe\", kafka_topics)\n",
    "    .option(\"startingOffsets\",\"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Initializing the streaming\n",
    "\n",
    "for table in avaiable_tables:\n",
    "    current_stream = create_postgres_delta_cdc_table(table, df_cdc_read_stream=df_cdc_read_stream,table_properties=table_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Stupid monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_streams = [{'id': val.id,'name':val.name,'last_batch':val.lastProgress} for val in spark.streams.active]\n",
    "iter_active_streams  = cycle(active_streams)\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.2)\n",
    "    current_stream_data = next(iter_active_streams)\n",
    "    current_stream = spark.streams.get(current_stream_data.get('id'))\n",
    "    \n",
    "    if not current_stream.isActive:\n",
    "        logger.warn(f' ERROR {current_stream.name} stopped for some reason !')\n",
    "\n",
    "    if (current_stream.lastProgress is not None) & (current_stream_data.get('last_batch') is not None): \n",
    "        if current_stream.lastProgress.get('batchId') > current_stream_data.get('last_batch').get('batchId'):\n",
    "            logger.debug(f'last process - {current_stream.name}  |  {current_stream.lastProgress}')\n",
    "            current_stream_data.update({'last_batch':current_stream.lastProgress})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.format('delta').load('/app/data/exampledb.public.bounties_fact/').orderBy(F.col('character_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.format('delta').load('/app/data/exampledb.public.characters_dim/').orderBy(F.col('character_id').asc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
